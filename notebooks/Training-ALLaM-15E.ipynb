{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6355eea0",
   "metadata": {},
   "source": [
    "## Training ALLaM using LoRA + Tag, 15EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT CHECK\n",
      "Python executable: /usr/bin/python\n",
      "Python version   : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Torch version    : 2.2.1+cu121\n",
      "Transformers ver : 4.42.3\n",
      "CUDA available   : True\n",
      "CUDA device count: 1\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation\n",
      "Datasets version : 2.19.1\n",
      "PEFT version     : 0.11.1\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8566f0cbb74a589a50bbe7d3a82139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ea671276bc44618b824b1d6c535ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 5,122 | Dev examples: 588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065821047a144b74a056513bc0fb733b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 79,953,920 || all params: 7,080,513,536 || trainable%: 1.1292\n",
      "Starting training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 45:31, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.121600</td>\n",
       "      <td>2.714053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.593600</td>\n",
       "      <td>2.235334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.049200</td>\n",
       "      <td>2.025661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.910300</td>\n",
       "      <td>1.952053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.831500</td>\n",
       "      <td>1.909792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.699600</td>\n",
       "      <td>1.895119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.645500</td>\n",
       "      <td>1.892531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1.921903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.444200</td>\n",
       "      <td>1.952040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.387200</td>\n",
       "      <td>1.965450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.359600</td>\n",
       "      <td>1.978206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.355400</td>\n",
       "      <td>1.979035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>1.979285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>1.979174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=0.07 train_loss=3.3063 eval_loss=N/A lr=2.5e-06 dt=13.34s elapsed=0.22m\n",
      "[LOG] epoch=0.72 train_loss=3.1216 eval_loss=N/A lr=2.5e-05 dt=119.87s elapsed=2.22m\n",
      "[LOG] epoch=0.94 train_loss=N/A eval_loss=2.714052677154541 lr=N/A dt=58.16s elapsed=3.19m\n",
      "[EVAL] epoch=0.94 eval_loss=2.7141 dt=0.00s elapsed=3.19m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=1.44 train_loss=2.5936 eval_loss=N/A lr=5e-05 dt=84.19s elapsed=4.59m\n",
      "[LOG] epoch=1.95 train_loss=N/A eval_loss=2.2353343963623047 lr=N/A dt=110.34s elapsed=6.43m\n",
      "[EVAL] epoch=1.95 eval_loss=2.2353 dt=0.00s elapsed=6.43m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=2.16 train_loss=2.2427 eval_loss=N/A lr=4.959823971496574e-05 dt=32.39s elapsed=6.97m\n",
      "[LOG] epoch=2.88 train_loss=2.0492 eval_loss=N/A lr=4.8405871765993433e-05 dt=134.30s elapsed=9.21m\n",
      "[LOG] epoch=2.95 train_loss=N/A eval_loss=2.025660991668701 lr=N/A dt=27.86s elapsed=9.67m\n",
      "[EVAL] epoch=2.95 eval_loss=2.0257 dt=0.00s elapsed=9.67m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=3.60 train_loss=1.9103 eval_loss=N/A lr=4.6461219840046654e-05 dt=114.26s elapsed=11.58m\n",
      "[LOG] epoch=3.96 train_loss=N/A eval_loss=1.9520529508590698 lr=N/A dt=80.01s elapsed=12.91m\n",
      "[EVAL] epoch=3.96 eval_loss=1.9521 dt=0.00s elapsed=12.91m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=4.32 train_loss=1.8315 eval_loss=N/A lr=4.382678665009028e-05 dt=63.26s elapsed=13.97m\n",
      "[LOG] epoch=4.97 train_loss=N/A eval_loss=1.9097920656204224 lr=N/A dt=131.91s elapsed=16.16m\n",
      "[EVAL] epoch=4.97 eval_loss=1.9098 dt=0.00s elapsed=16.17m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=5.05 train_loss=1.7707 eval_loss=N/A lr=4.058724504646834e-05 dt=9.92s elapsed=16.33m\n",
      "[LOG] epoch=5.77 train_loss=1.6996 eval_loss=N/A lr=3.6846716561824965e-05 dt=134.61s elapsed=18.57m\n",
      "[LOG] epoch=5.98 train_loss=N/A eval_loss=1.8951194286346436 lr=N/A dt=49.86s elapsed=19.40m\n",
      "[EVAL] epoch=5.98 eval_loss=1.8951 dt=0.00s elapsed=19.41m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=6.49 train_loss=1.6455 eval_loss=N/A lr=3.272542485937369e-05 dt=94.37s elapsed=20.98m\n",
      "[LOG] epoch=6.99 train_loss=N/A eval_loss=1.892530918121338 lr=N/A dt=101.95s elapsed=22.68m\n",
      "[EVAL] epoch=6.99 eval_loss=1.8925 dt=0.00s elapsed=22.68m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=7.21 train_loss=1.5687 eval_loss=N/A lr=2.8355831645441388e-05 dt=40.42s elapsed=23.35m\n",
      "[LOG] epoch=7.93 train_loss=1.5355 eval_loss=N/A lr=2.3878379241237136e-05 dt=134.45s elapsed=25.59m\n",
      "[LOG] epoch=8.00 train_loss=N/A eval_loss=1.9024438858032227 lr=N/A dt=19.46s elapsed=25.92m\n",
      "[EVAL] epoch=8.00 eval_loss=1.9024 dt=0.00s elapsed=25.92m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=8.65 train_loss=1.48 eval_loss=N/A lr=1.9436976651092144e-05 dt=122.94s elapsed=27.96m\n",
      "[LOG] epoch=8.94 train_loss=N/A eval_loss=1.9219032526016235 lr=N/A dt=71.56s elapsed=29.16m\n",
      "[EVAL] epoch=8.94 eval_loss=1.9219 dt=0.00s elapsed=29.16m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=9.37 train_loss=1.4442 eval_loss=N/A lr=1.5174374208651912e-05 dt=70.75s elapsed=30.34m\n",
      "[LOG] epoch=9.95 train_loss=N/A eval_loss=1.9520400762557983 lr=N/A dt=123.78s elapsed=32.40m\n",
      "[EVAL] epoch=9.95 eval_loss=1.9520 dt=0.00s elapsed=32.40m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=10.09 train_loss=1.4222 eval_loss=N/A lr=1.122757546369744e-05 dt=18.77s elapsed=32.71m\n",
      "[LOG] epoch=10.81 train_loss=1.3872 eval_loss=N/A lr=7.723433775328384e-06 dt=134.60s elapsed=34.96m\n",
      "[LOG] epoch=10.95 train_loss=N/A eval_loss=1.9654499292373657 lr=N/A dt=41.36s elapsed=35.65m\n",
      "[EVAL] epoch=10.95 eval_loss=1.9654 dt=0.00s elapsed=35.65m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=11.53 train_loss=1.3596 eval_loss=N/A lr=4.7745751406263165e-06 dt=100.96s elapsed=37.33m\n",
      "[LOG] epoch=11.96 train_loss=N/A eval_loss=1.9782055616378784 lr=N/A dt=93.45s elapsed=38.89m\n",
      "[EVAL] epoch=11.96 eval_loss=1.9782 dt=0.00s elapsed=38.89m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=12.25 train_loss=1.3666 eval_loss=N/A lr=2.475778302439524e-06 dt=48.66s elapsed=39.70m\n",
      "[LOG] epoch=12.97 train_loss=1.3554 eval_loss=N/A lr=9.009284826036691e-07 dt=134.55s elapsed=41.94m\n",
      "[LOG] epoch=12.97 train_loss=N/A eval_loss=1.9790353775024414 lr=N/A dt=11.06s elapsed=42.12m\n",
      "[EVAL] epoch=12.97 eval_loss=1.9790 dt=0.00s elapsed=42.12m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=13.69 train_loss=1.3529 eval_loss=N/A lr=1.006426501190233e-07 dt=131.02s elapsed=44.31m\n",
      "[LOG] epoch=13.98 train_loss=N/A eval_loss=1.9792847633361816 lr=N/A dt=63.19s elapsed=45.36m\n",
      "[EVAL] epoch=13.98 eval_loss=1.9793 dt=0.00s elapsed=45.36m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=14.05 train_loss=N/A eval_loss=1.9791737794876099 lr=N/A dt=20.92s elapsed=45.71m\n",
      "[EVAL] epoch=14.05 eval_loss=1.9792 dt=0.00s elapsed=45.71m\n",
      "✅ Done. LoRA adapters saved to: outputs/allam7b-lora-token-15EPOCH\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=14.05 train_loss=N/A eval_loss=1.892530918121338 lr=N/A dt=9.95s elapsed=45.87m\n",
      "[EVAL] epoch=14.05 eval_loss=1.8925 dt=0.00s elapsed=45.87m\n",
      "Final evaluation metrics: {'eval_loss': 1.892530918121338, 'eval_runtime': 6.8253, 'eval_samples_per_second': 3.663, 'eval_steps_per_second': 0.586, 'epoch': 14.054054054054054}\n",
      "Perplexity: 6.636\n",
      "Saved run_config.txt\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time, random, numpy as np, torch, transformers\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import set_seed\n",
    "\n",
    "# -----------------------------\n",
    "# ENV + Reproducibility\n",
    "# -----------------------------\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version   : {sys.version}\")\n",
    "print(f\"Torch version    : {torch.__version__}\")\n",
    "print(f\"Transformers ver : {transformers.__version__}\")\n",
    "print(f\"CUDA available   : {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA device detected!\")\n",
    "try:\n",
    "    import datasets; print(f\"Datasets version : {datasets.__version__}\")\n",
    "except Exception: print(\"Datasets not installed?\")\n",
    "try:\n",
    "    import peft; print(f\"PEFT version     : {peft.__version__}\")\n",
    "except Exception: print(\"PEFT not installed?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -----------------------------\n",
    "# PATHS / MODEL\n",
    "# -----------------------------\n",
    "TRAIN_PATH = Path(\"data_splits/train.jsonl\")\n",
    "DEV_PATH   = Path(\"data_splits/dev.jsonl\")\n",
    "OUTPUT_DIR = Path(\"outputs/allam7b-lora-token-15EPOCH\")\n",
    "BASE_MODEL = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "MAX_SEQ_LEN = 2048 \n",
    "\n",
    "# -----------------------------\n",
    "# LoRA config\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  \n",
    "    lora_alpha=64,  \n",
    "    lora_dropout=0.1,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training args (TRL SFTConfig; TRL==0.9.6)\n",
    "# ----------------------------\n",
    "\n",
    "train_args = SFTConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=15,  \n",
    "    per_device_train_batch_size=2, \n",
    "    gradient_accumulation_steps=8,  \n",
    "    learning_rate=5e-5,  \n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1,  \n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3,  # Keep more checkpoints\n",
    "    seed=SEED,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    # Add these for better logging\n",
    "    logging_dir=str(OUTPUT_DIR / \"logs\"),\n",
    "    logging_first_step=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "# -----------------------------\n",
    "# Data IO\n",
    "# -----------------------------\n",
    "def load_jsonl(path: Path) -> Dataset:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            obj = json.loads(line)\n",
    "            instr = (obj.get(\"instruction\") or \"\").strip()\n",
    "            resp  = (obj.get(\"response\") or \"\").strip()\n",
    "            if instr and resp:\n",
    "                rows.append({\"instruction\": instr, \"response\": resp})\n",
    "    if not rows:\n",
    "        raise ValueError(f\"No valid rows found in {path}\")\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_ds = load_jsonl(TRAIN_PATH)\n",
    "dev_ds   = load_jsonl(DEV_PATH)\n",
    "\n",
    "EOS = \"</s>\"\n",
    "def fmt(example):\n",
    "    instr = example[\"instruction\"] \n",
    "    resp  = example[\"response\"]\n",
    "    text = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}{EOS}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_text = train_ds.map(fmt, remove_columns=[c for c in train_ds.column_names if c!=\"text\"])\n",
    "dev_text   = dev_ds.map(fmt,   remove_columns=[c for c in dev_ds.column_names if c!=\"text\"])\n",
    "\n",
    "print(f\"Train examples: {len(train_text):,} | Dev examples: {len(dev_text):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer & Base model\n",
    "# -----------------------------\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# -----------------------------\n",
    "# Step logging callback\n",
    "# -----------------------------\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()\n",
    "        self.tlast = self.t0\n",
    "\n",
    "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "        if not state.is_local_process_zero:\n",
    "            return\n",
    "        logs = logs or {}\n",
    "        now = time.time()\n",
    "        train_loss = logs.get(\"loss\")\n",
    "        if train_loss is not None:\n",
    "            print(\n",
    "                f\"[Step {state.global_step}] epoch={state.epoch:.2f} \"\n",
    "                f\"train_loss={train_loss:.4f} \"\n",
    "                f\"lr={logs.get('learning_rate', 'N/A'):.2e} \"\n",
    "                f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                flush=True\n",
    "            )\n",
    "            self.tlast = now\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if not state.is_local_process_zero:\n",
    "            return\n",
    "        metrics = metrics or {}\n",
    "        now = time.time()\n",
    "        \n",
    "        eval_loss = metrics.get(\"eval_loss\")\n",
    "        if eval_loss is not None:\n",
    "            print(\n",
    "                f\"[EVAL] epoch={state.epoch:.2f} \"\n",
    "                f\"eval_loss={eval_loss:.4f} \"\n",
    "                f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                flush=True\n",
    "            )\n",
    "            self.tlast = now\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not state.is_local_process_zero:\n",
    "            return\n",
    "        logs = logs or {}\n",
    "        now = time.time()\n",
    "        \n",
    "        # Log any metrics that weren't caught above\n",
    "        if \"loss\" in logs or \"eval_loss\" in logs:\n",
    "            train_loss = logs.get(\"loss\", \"N/A\")\n",
    "            eval_loss = logs.get(\"eval_loss\", \"N/A\")\n",
    "            lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            \n",
    "            print(\n",
    "                f\"[LOG] epoch={state.epoch:.2f} \"\n",
    "                f\"train_loss={train_loss} \"\n",
    "                f\"eval_loss={eval_loss} \"\n",
    "                f\"lr={lr} \"\n",
    "                f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                flush=True\n",
    "            )\n",
    "            self.tlast = now\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer (TRL + LoRA)\n",
    "# -----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,  \n",
    "    train_dataset=train_text,\n",
    "    eval_dataset=dev_text,\n",
    "    peft_config=lora_config,\n",
    "    args=train_args,\n",
    "    max_seq_length=MAX_SEQ_LEN,  \n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer.model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "trainer.add_callback(LoggingCallback())\n",
    "\n",
    "print(\"Starting training…\", flush=True)\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# Save adapters & tokenizer\n",
    "# -----------------------------\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model()\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(\"✅ Done. LoRA adapters saved to:\", OUTPUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Final dev evaluation + perplexity\n",
    "# -----------------------------\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final evaluation metrics:\", metrics)\n",
    "if \"eval_loss\" in metrics and metrics[\"eval_loss\"] is not None:\n",
    "    try:\n",
    "        ppl = float(np.exp(metrics[\"eval_loss\"]))\n",
    "        print(f\"Perplexity: {ppl:.3f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# Save run config snapshot\n",
    "# -----------------------------\n",
    "with open(OUTPUT_DIR / \"run_config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Seed: {SEED}\\n\")\n",
    "    f.write(f\"Base model: {BASE_MODEL}\\n\")\n",
    "    f.write(f\"Train path: {TRAIN_PATH}\\n\")\n",
    "    f.write(f\"Dev path  : {DEV_PATH}\\n\")\n",
    "    f.write(f\"Epochs: {train_args.num_train_epochs}\\n\")\n",
    "    f.write(f\"LR: {train_args.learning_rate}\\n\")\n",
    "    f.write(f\"Per-device batch: {train_args.per_device_train_batch_size}\\n\")\n",
    "    f.write(f\"Grad accum: {train_args.gradient_accumulation_steps}\\n\")\n",
    "    f.write(f\"Max seq len: {MAX_SEQ_LEN}\\n\")\n",
    "    f.write(f\"BF16: {train_args.bf16}\\n\")\n",
    "    f.write(f\"LORA r/alpha/drop: {lora_config.r}/{lora_config.lora_alpha}/{lora_config.lora_dropout}\\n\")\n",
    "    f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
    "    f.write(f\"Torch: {torch.__version__}\\n\")\n",
    "print(\"Saved run_config.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc17c8df-03a9-42e0-8fc1-253362eb3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval_loss: 1.892530918121338\n",
      "Best checkpoint path: outputs/allam7b-lora-token-15EPOCH/checkpoint-97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best eval_loss:\", trainer.state.best_metric)\n",
    "print(\"Best checkpoint path:\", trainer.state.best_model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175ffab",
   "metadata": {},
   "source": [
    "## Training ALLaM using LoRA + No Tag, 15EPOCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NO-TAG TRAIN — ENVIRONMENT CHECK\n",
      "Python executable: /usr/bin/python\n",
      "Python version   : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Torch version    : 2.2.1+cu121\n",
      "Transformers ver : 4.42.3\n",
      "CUDA available   : True\n",
      "CUDA device count: 1\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation\n",
      "Datasets version : 2.19.1\n",
      "PEFT version     : 0.11.1\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcd668a453d4240979897ac4550c50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fce9c3fc4d1411fbf1c01de78b3f6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 5,122 | Dev examples: 588\n",
      "Leading <DIALECT=…> tags removed in formatting: 5710/5710 (100.00%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1692c9a98782418199fa9cad9db83b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 79,953,920 || all params: 7,080,513,536 || trainable%: 1.1292\n",
      "Starting NO-TAG training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 42:16, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.436900</td>\n",
       "      <td>3.009303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.837000</td>\n",
       "      <td>2.551401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.506300</td>\n",
       "      <td>2.285813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.257300</td>\n",
       "      <td>2.190359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.032000</td>\n",
       "      <td>2.138714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.938700</td>\n",
       "      <td>2.108704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.882600</td>\n",
       "      <td>2.100609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.698200</td>\n",
       "      <td>2.122641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.649100</td>\n",
       "      <td>2.148299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.596300</td>\n",
       "      <td>2.162692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.606500</td>\n",
       "      <td>2.173975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.563400</td>\n",
       "      <td>2.177813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.561700</td>\n",
       "      <td>2.177822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.552200</td>\n",
       "      <td>2.177547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=0.08 train_loss=3.6483 eval_loss=N/A lr=2.777777777777778e-06 dt=13.33s elapsed=0.22m\n",
      "[LOG] epoch=0.81 train_loss=3.4369 eval_loss=N/A lr=2.777777777777778e-05 dt=119.62s elapsed=2.22m\n",
      "[LOG] epoch=0.97 train_loss=N/A eval_loss=3.0093026161193848 lr=N/A dt=38.37s elapsed=2.86m\n",
      "[EVAL] epoch=0.97 eval_loss=3.0093 dt=0.00s elapsed=2.86m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=1.62 train_loss=2.837 eval_loss=N/A lr=4.998119881260576e-05 dt=104.55s elapsed=4.60m\n",
      "[LOG] epoch=1.94 train_loss=N/A eval_loss=2.551401138305664 lr=N/A dt=70.45s elapsed=5.77m\n",
      "[EVAL] epoch=1.94 eval_loss=2.5514 dt=0.00s elapsed=5.77m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=2.42 train_loss=2.5063 eval_loss=N/A lr=4.9326121764495596e-05 dt=75.09s elapsed=7.02m\n",
      "[LOG] epoch=2.99 train_loss=N/A eval_loss=2.285813093185425 lr=N/A dt=102.32s elapsed=8.73m\n",
      "[EVAL] epoch=2.99 eval_loss=2.2858 dt=0.00s elapsed=8.73m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=3.23 train_loss=2.2573 eval_loss=N/A lr=4.775907352415367e-05 dt=40.26s elapsed=9.40m\n",
      "[LOG] epoch=3.96 train_loss=N/A eval_loss=2.190358877182007 lr=N/A dt=134.23s elapsed=11.64m\n",
      "[EVAL] epoch=3.96 eval_loss=2.1904 dt=0.00s elapsed=11.64m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=4.04 train_loss=2.1158 eval_loss=N/A lr=4.533880175657419e-05 dt=9.00s elapsed=11.79m\n",
      "[LOG] epoch=4.85 train_loss=2.032 eval_loss=N/A lr=4.215604094671835e-05 dt=134.39s elapsed=14.03m\n",
      "[LOG] epoch=4.93 train_loss=N/A eval_loss=2.1387135982513428 lr=N/A dt=31.59s elapsed=14.55m\n",
      "[EVAL] epoch=4.93 eval_loss=2.1387 dt=0.00s elapsed=14.55m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=5.66 train_loss=1.9387 eval_loss=N/A lr=3.8330110820042285e-05 dt=111.91s elapsed=16.42m\n",
      "[LOG] epoch=5.98 train_loss=N/A eval_loss=2.10870361328125 lr=N/A dt=63.63s elapsed=17.48m\n",
      "[EVAL] epoch=5.98 eval_loss=2.1087 dt=0.00s elapsed=17.48m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=6.46 train_loss=1.8826 eval_loss=N/A lr=3.400444312011776e-05 dt=79.24s elapsed=18.80m\n",
      "[LOG] epoch=6.95 train_loss=N/A eval_loss=2.1006088256835938 lr=N/A dt=95.51s elapsed=20.39m\n",
      "[EVAL] epoch=6.95 eval_loss=2.1006 dt=0.00s elapsed=20.39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=7.27 train_loss=1.7975 eval_loss=N/A lr=2.9341204441673266e-05 dt=47.36s elapsed=21.18m\n",
      "[LOG] epoch=8.00 train_loss=N/A eval_loss=2.110795736312866 lr=N/A dt=127.56s elapsed=23.31m\n",
      "[EVAL] epoch=8.00 eval_loss=2.1108 dt=0.00s elapsed=23.31m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=8.08 train_loss=1.7476 eval_loss=N/A lr=2.4515216705704395e-05 dt=15.33s elapsed=23.56m\n",
      "[LOG] epoch=8.89 train_loss=1.6982 eval_loss=N/A lr=1.970740319426474e-05 dt=134.44s elapsed=25.80m\n",
      "[LOG] epoch=8.97 train_loss=N/A eval_loss=2.122641086578369 lr=N/A dt=24.91s elapsed=26.22m\n",
      "[EVAL] epoch=8.97 eval_loss=2.1226 dt=0.00s elapsed=26.22m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=9.70 train_loss=1.6491 eval_loss=N/A lr=1.509800584902108e-05 dt=117.93s elapsed=28.18m\n",
      "[LOG] epoch=9.94 train_loss=N/A eval_loss=2.148298740386963 lr=N/A dt=56.90s elapsed=29.13m\n",
      "[EVAL] epoch=9.94 eval_loss=2.1483 dt=0.00s elapsed=29.13m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=10.51 train_loss=1.5963 eval_loss=N/A lr=1.085982811283654e-05 dt=85.72s elapsed=30.56m\n",
      "[LOG] epoch=10.99 train_loss=N/A eval_loss=2.1626923084259033 lr=N/A dt=88.95s elapsed=32.04m\n",
      "[EVAL] epoch=10.99 eval_loss=2.1627 dt=0.00s elapsed=32.04m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=11.31 train_loss=1.6065 eval_loss=N/A lr=7.1517566360525284e-06 dt=53.96s elapsed=32.94m\n",
      "[LOG] epoch=11.96 train_loss=N/A eval_loss=2.1739745140075684 lr=N/A dt=120.98s elapsed=34.96m\n",
      "[EVAL] epoch=11.96 eval_loss=2.1740 dt=0.00s elapsed=34.96m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=12.12 train_loss=1.555 eval_loss=N/A lr=4.112804714676594e-06 dt=21.83s elapsed=35.32m\n",
      "[LOG] epoch=12.93 train_loss=1.5634 eval_loss=N/A lr=1.8569007682777417e-06 dt=134.67s elapsed=37.57m\n",
      "[LOG] epoch=12.93 train_loss=N/A eval_loss=2.1778132915496826 lr=N/A dt=18.21s elapsed=37.87m\n",
      "[EVAL] epoch=12.93 eval_loss=2.1778 dt=0.00s elapsed=37.87m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=13.74 train_loss=1.5617 eval_loss=N/A lr=4.6861723431538276e-07 dt=124.48s elapsed=39.95m\n",
      "[LOG] epoch=13.98 train_loss=N/A eval_loss=2.1778223514556885 lr=N/A dt=50.17s elapsed=40.78m\n",
      "[EVAL] epoch=13.98 eval_loss=2.1778 dt=0.00s elapsed=40.78m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=14.55 train_loss=1.5522 eval_loss=N/A lr=0.0 dt=92.55s elapsed=42.32m\n",
      "[LOG] epoch=14.55 train_loss=N/A eval_loss=2.177546501159668 lr=N/A dt=8.13s elapsed=42.46m\n",
      "[EVAL] epoch=14.55 eval_loss=2.1775 dt=0.00s elapsed=42.46m\n",
      "✅ Done. LoRA (NO TAG) adapters saved to: outputs/allam7b-lora-no-token-15EPOCH\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] epoch=14.55 train_loss=N/A eval_loss=2.1006088256835938 lr=N/A dt=9.82s elapsed=42.62m\n",
      "[EVAL] epoch=14.55 eval_loss=2.1006 dt=0.00s elapsed=42.62m\n",
      "Final evaluation metrics: {'eval_loss': 2.1006088256835938, 'eval_runtime': 6.3647, 'eval_samples_per_second': 3.614, 'eval_steps_per_second': 0.471, 'epoch': 14.545454545454545}\n",
      "Perplexity: 8.171\n",
      "Saved run_config.txt\n"
     ]
    }
   ],
   "source": [
    "    # -----------------------------\n",
    "    # PATHS / MODEL\n",
    "    # -----------------------------\n",
    "    TRAIN_PATH = Path(\"data_splits/train.jsonl\")\n",
    "    DEV_PATH   = Path(\"data_splits/dev.jsonl\")\n",
    "    OUTPUT_DIR = Path(\"outputs/allam7b-lora-no-token-15EPOCH\")\n",
    "    BASE_MODEL = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "    MAX_SEQ_LEN = 2048\n",
    "\n",
    "    # -----------------------------\n",
    "    # LoRA config\n",
    "    # -----------------------------\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,  \n",
    "        lora_alpha=64,  \n",
    "        lora_dropout=0.1,  \n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training args (TRL SFTConfig; TRL==0.9.6)\n",
    "    # -----------------------------\n",
    "\n",
    "    train_args = SFTConfig(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        num_train_epochs=15,  \n",
    "        per_device_train_batch_size=2,  \n",
    "        gradient_accumulation_steps=8,  \n",
    "        learning_rate=5e-5, \n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        warmup_ratio=0.1,  \n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_total_limit=3,  \n",
    "        seed=SEED,\n",
    "        packing=True,\n",
    "        dataset_text_field=\"text\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        logging_dir=str(OUTPUT_DIR / \"logs\"),\n",
    "        logging_first_step=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    # -----------------------------\n",
    "    # Data IO\n",
    "    # -----------------------------\n",
    "    def load_jsonl(path: Path) -> Dataset:\n",
    "        rows = []\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                instr = (obj.get(\"instruction\") or \"\").strip()\n",
    "                resp  = (obj.get(\"response\") or \"\").strip()\n",
    "                if instr and resp:\n",
    "                    rows.append({\"instruction\": instr, \"response\": resp})\n",
    "        if not rows:\n",
    "            raise ValueError(f\"No valid rows found in {path}\")\n",
    "        return Dataset.from_list(rows)\n",
    "\n",
    "    train_ds = load_jsonl(TRAIN_PATH)\n",
    "    dev_ds   = load_jsonl(DEV_PATH)\n",
    "    TAG_RE = re.compile(r'^\\s*<\\s*DIALECT\\s*=\\s*(HIJAZI|NAJDI)\\s*>\\s*', re.IGNORECASE)\n",
    "    EOS = \"</s>\"\n",
    "\n",
    "    removed_ct = 0\n",
    "    total_ct   = 0\n",
    "    def fmt_no_tag(example):\n",
    "        global removed_ct, total_ct\n",
    "        instr = example[\"instruction\"]\n",
    "        total_ct += 1\n",
    "        stripped = TAG_RE.sub(\"\", instr).lstrip()\n",
    "        if stripped != instr:\n",
    "            removed_ct += 1\n",
    "        resp = example[\"response\"]\n",
    "        text = f\"### Instruction:\\n{stripped}\\n\\n### Response:\\n{resp}{EOS}\"\n",
    "        return {\"text\": text}\n",
    "\n",
    "    train_text = train_ds.map(fmt_no_tag, remove_columns=[c for c in train_ds.column_names if c!=\"text\"])\n",
    "    dev_text   = dev_ds.map(fmt_no_tag,   remove_columns=[c for c in dev_ds.column_names if c!=\"text\"])\n",
    "\n",
    "    print(f\"Train examples: {len(train_text):,} | Dev examples: {len(dev_text):,}\")\n",
    "    print(f\"Leading <DIALECT=…> tags removed in formatting: {removed_ct}/{total_ct} \"\n",
    "        f\"({(removed_ct/max(1,total_ct))*100:.2f}%)\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Tokenizer & Base model\n",
    "    # -----------------------------\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step logging\n",
    "    # -----------------------------\n",
    "    class LoggingCallback(TrainerCallback):\n",
    "        def __init__(self):\n",
    "            self.t0 = time.time()\n",
    "            self.tlast = self.t0\n",
    "\n",
    "        def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "            \"\"\"Log every training step\"\"\"\n",
    "            if not state.is_local_process_zero:\n",
    "                return\n",
    "            logs = logs or {}\n",
    "            now = time.time()\n",
    "            train_loss = logs.get(\"loss\")\n",
    "            if train_loss is not None:\n",
    "                print(\n",
    "                    f\"[Step {state.global_step}] epoch={state.epoch:.2f} \"\n",
    "                    f\"train_loss={train_loss:.4f} \"\n",
    "                    f\"lr={logs.get('learning_rate', 'N/A'):.2e} \"\n",
    "                    f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                    flush=True\n",
    "                )\n",
    "                self.tlast = now\n",
    "\n",
    "        def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "            \"\"\"Log evaluation results\"\"\"\n",
    "            if not state.is_local_process_zero:\n",
    "                return\n",
    "            metrics = metrics or {}\n",
    "            now = time.time()\n",
    "            \n",
    "            eval_loss = metrics.get(\"eval_loss\")\n",
    "            if eval_loss is not None:\n",
    "                print(\n",
    "                    f\"[EVAL] epoch={state.epoch:.2f} \"\n",
    "                    f\"eval_loss={eval_loss:.4f} \"\n",
    "                    f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                    flush=True\n",
    "                )\n",
    "                self.tlast = now\n",
    "\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            \"\"\"Fallback logging\"\"\"\n",
    "            if not state.is_local_process_zero:\n",
    "                return\n",
    "            logs = logs or {}\n",
    "            now = time.time()\n",
    "            if \"loss\" in logs or \"eval_loss\" in logs:\n",
    "                train_loss = logs.get(\"loss\", \"N/A\")\n",
    "                eval_loss = logs.get(\"eval_loss\", \"N/A\")\n",
    "                lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "                \n",
    "                print(\n",
    "                    f\"[LOG] epoch={state.epoch:.2f} \"\n",
    "                    f\"train_loss={train_loss} \"\n",
    "                    f\"eval_loss={eval_loss} \"\n",
    "                    f\"lr={lr} \"\n",
    "                    f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                    flush=True\n",
    "                )\n",
    "                self.tlast = now\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trainer (TRL + LoRA)\n",
    "    # -----------------------------\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tok,  \n",
    "        train_dataset=train_text,\n",
    "        eval_dataset=dev_text,\n",
    "        peft_config=lora_config,\n",
    "        args=train_args,\n",
    "        max_seq_length=MAX_SEQ_LEN,  \n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    trainer.add_callback(LoggingCallback())\n",
    "    print(\"Starting NO-TAG training…\", flush=True)\n",
    "    trainer.train()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save adapters & tokenizer\n",
    "    # -----------------------------\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.save_model()\n",
    "    tok.save_pretrained(OUTPUT_DIR)\n",
    "    print(\"✅ Done. LoRA (NO TAG) adapters saved to:\", OUTPUT_DIR)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final dev evaluation + perplexity\n",
    "    # -----------------------------\n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"Final evaluation metrics:\", metrics)\n",
    "    if \"eval_loss\" in metrics and metrics[\"eval_loss\"] is not None:\n",
    "        try:\n",
    "            ppl = float(np.exp(metrics[\"eval_loss\"]))\n",
    "            print(f\"Perplexity: {ppl:.3f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save run config snapshot\n",
    "    # -----------------------------\n",
    "    with open(OUTPUT_DIR / \"run_config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Seed: {SEED}\\n\")\n",
    "        f.write(f\"Base model: {BASE_MODEL}\\n\")\n",
    "        f.write(f\"Train path: {TRAIN_PATH}\\n\")\n",
    "        f.write(f\"Dev path  : {DEV_PATH}\\n\")\n",
    "        f.write(f\"Epochs: {train_args.num_train_epochs}\\n\")\n",
    "        f.write(f\"LR: {train_args.learning_rate}\\n\")\n",
    "        f.write(f\"Per-device batch: {train_args.per_device_train_batch_size}\\n\")\n",
    "        f.write(f\"Grad accum: {train_args.gradient_accumulation_steps}\\n\")\n",
    "        f.write(f\"Max seq len: {MAX_SEQ_LEN}\\n\")\n",
    "        f.write(f\"BF16: {train_args.bf16}\\n\")\n",
    "        f.write(f\"LORA r/alpha/drop: {lora_config.r}/{lora_config.lora_alpha}/{lora_config.lora_dropout}\\n\")\n",
    "        f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
    "        f.write(f\"Torch: {torch.__version__}\\n\")\n",
    "    print(\"Saved run_config.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2c8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval_loss: 2.1006088256835938\n",
      "Best checkpoint path: outputs/allam7b-lora-no-token-15EPOCH/checkpoint-86\n"
     ]
    }
   ],
   "source": [
    "print(\"Best eval_loss:\", trainer.state.best_metric)\n",
    "print(\"Best checkpoint path:\", trainer.state.best_model_checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
